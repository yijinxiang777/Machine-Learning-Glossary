---
layout: default
title: Machine Learning and Artificial Intelligence
parent: Introduction
has_children: true
nav_order: 1
---

# Machine Learning and Artificial Intelligence
{: .no_toc }

---

### Machine Learning and Artificial Intelligence  

Simply stated, "Artificial intelligence is not intelligence. Machine learning is not learning."[cite Burkov  (2019 p4)] What a typical "learning machine" does is find a math formula which, when applied to collection of inputs (training data), produces outputs [@Burkov2019]. Whether a particular implementation of a machine learning algorithm is successful or not is often marked by how “desirable” the outputs are. The “desirability” of the outputs of a ML algorithm is often defined using precise mathematics. As we discuss in more detail below, without a fundamental understanding of what these mathematical equations imply, researchers can easily misinterpret whether the outputs of a given ML algorithm are meaningfully useful or “optimal” in a practical sense. 
Understanding why these fields are termed “machine learning” and “artificial intelligence”, and the history of their development, may provide some protection against the hype and misplaced expectations of research and application in these areas. Machine learning and artificial intelligence were so named primarily for marketing reasons. For example, Arthur Samuel coined the term “machine learning” in 1959 while at IBM to attract clients and employees with the idea of cutting edge feats of technology.[cite the Mitchell book; is there something in Waldrop Complexity book?; can we find something by Arthur Samuel?]   

ML and AI also developed in different contexts and with different goals. The roots of machine learning originate in the pattern recognition world. Loosely, pattern recognition consists of employing a dataset and an algorithm to discover generalizable patterns that can be used for some productive purpose, such as to inform decision making, for knowledge discovery, or for robotic automation. [example for ML? IBM example?] Artificial intelligence, on the other hand, has its roots in symbolic logic and symbol manipulation. Instead of relying on data, the roots of AI began with computer programs that were written with a particular intent. One of the first examples is the General Problem Solver developed by Herbert Simon and Allen Newell, which consisted of a series of general programmatic if-else statements that would be deployed to find optimal solutions to decision-making problems, such as the “Missionaries and Cannibals” problem or the “prisoner’s dilemma” [see page 22 of @Mitchell2019].
Furthermore, the goals of AI have long been explicit, and are generally divided into two categories: general or strong AI and narrow or specific AI. The objective of general AI is to develop a machine that has patterns of “behavior” similar to that of human beings (e.g., develop, learn, understand, interact). As of yet, researchers have not developed such a machine. However, some researchers anticipate an “AI singularity,” where AI will be able to learn by itself, thus enabling machines to exceed all aspects of human-level intelligence. In contrast, narrow or specific AI, generally refers to machines that are devoted to doing one specific thing well, including, but not limited to, Deep Blue (the IBM chess computer), car manufacturing robots, and virtual assistants (ie. Siri, Cortana, Alexa).  

Despite their distinct origins, today ML and AI share a lot in common. Importantly, there are no generally accepted definitions of machine learning and artificial intelligence. AI is often described as a "branch of computer science that studies the properties of intelligence by synthesizing intelligence" [@AI100]. ML is often described as the "automated detection of meaningful patterns in data"]. Yet, in their modern forms, it is difficult to identify differences between these two areas of research and application. Both areas have been heavily influenced by developments in neural networks and deep learning, while ML or AI based on symbol manipulation are no longer a focus of research and application.
